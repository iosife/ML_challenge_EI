{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Matches the entries of two datasets. \"\"\"\n",
    "\n",
    "import sys\n",
    "import logging as lg\n",
    "import re\n",
    "import difflib\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "lg.basicConfig(format='%(asctime)s - %(message)s', level=lg.INFO)\n",
    "\n",
    "class Matcher:\n",
    "    \"\"\"\n",
    "    Matches the entries of two datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset1_filename: str, dataset2_filename: str) -> None:\n",
    "        \"\"\"\n",
    "        1st arg: Filename of the 1st dataset.\n",
    "        2st arg: Filename of the 2nd dataset.\n",
    "        Returns: None.\n",
    "        \"\"\"\n",
    "        self.dataset1_filename = dataset1_filename\n",
    "        self.dataset2_filename = dataset2_filename\n",
    "        self.dataset1 = pd.DataFrame()\n",
    "        self.dataset2 = pd.DataFrame()\n",
    "        self.matches = {}\n",
    "        self.full_matches = 0\n",
    "        self.partial_matches = 0\n",
    "        self.evaluation_f1 = 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def load_dataset(dat_fname: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads the two datasets.\n",
    "        1st arg: The filename of the dataset\n",
    "        Returns: The dataset as pandas datframe.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tsv_dataset = pd.read_csv(dat_fname, sep='\\t',\\\n",
    "                error_bad_lines=False, warn_bad_lines=False)\n",
    "        except IOError:\n",
    "            lg.critical(\"Fatal error: dataset cannot be loaded.\")\n",
    "            sys.exit(\"Fatal error: dataset cannot be loaded.\")\n",
    "        return tsv_dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def estimate_match_score(str1: str, str2: str) -> float:\n",
    "        \"\"\"\n",
    "        Estimates a score indicating the lexical similarity (match)\n",
    "        of two strings.\n",
    "        1st arg: The first string.\n",
    "        2nd arg: The second string.\n",
    "        Returns: A similarity score (high means high similarity).\n",
    "        \"\"\"\n",
    "        str_similarity = difflib.SequenceMatcher(None, str1, str2).ratio()\n",
    "        str_similarity = (\"%.2f\" % str_similarity)\n",
    "        return str_similarity\n",
    "\n",
    "    def preprocess_datasets(self) -> None:\n",
    "        \"\"\"\n",
    "        Preprocessing for both dataset:\n",
    "        (i) Removal of duplicate entries.\n",
    "        (ii) Substitution of missing fields with fixed negative values.\n",
    "        (iii) Definition of data types for dataframes.\n",
    "        (iv) Conversion to lower case.\n",
    "        Returns: None.\n",
    "        \"\"\"\n",
    "        self.dataset1 = self.dataset1.drop_duplicates()\n",
    "        self.dataset2 = self.dataset2.drop_duplicates()\n",
    "        self.dataset1 = self.dataset1.fillna(-1)\n",
    "        self.dataset2 = self.dataset2.fillna(-2)\n",
    "\n",
    "        new_dtypes1 = {\"id\": int, \"name\": str, \"street_number\": int, \"street_type\": str,\\\n",
    "            \"street_name\": str, \"address_line2\": str, \"postal_code\": int, \"city\": str}\n",
    "        self.dataset1 = self.dataset1.astype(new_dtypes1)\n",
    "        new_dtypes2 = {\"address\": str, \"website\": str, \"id\": int, \"name\": str}\n",
    "        self.dataset2 = self.dataset2.astype(new_dtypes2)\n",
    "\n",
    "        self.dataset1['name'] = self.dataset1['name'].str.lower()\n",
    "        self.dataset1['street_type'] = self.dataset1['street_type'].str.lower()\n",
    "        self.dataset1['street_name'] = self.dataset1['street_name'].str.lower()\n",
    "        self.dataset1['address_line2'] = self.dataset1['address_line2'].str.lower()\n",
    "        self.dataset1['city'] = self.dataset1['city'].str.lower()\n",
    "\n",
    "        self.dataset2['address'] = self.dataset2['address'].str.lower()\n",
    "        self.dataset2['website'] = self.dataset2['website'].str.lower()\n",
    "        self.dataset2['name'] = self.dataset2['name'].str.lower()\n",
    "\n",
    "    @staticmethod\n",
    "    def has_one_entry(matched_entries: pd.DataFrame) -> bool:\n",
    "        \"\"\"\n",
    "        Checks whether only one entry exists.\n",
    "        1st arg: Matched entry(ies) from a dataset.\n",
    "        Returns: Boolean.\n",
    "        \"\"\"\n",
    "        dframe = pd.DataFrame()\n",
    "        dframe = matched_entries\n",
    "        if dframe.shape[0] == 1:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def search_target_dataset(self, id_code: int, snum: str,\\\n",
    "        sname: str, postal_code_city: str) -> dict:\n",
    "        \"\"\"\n",
    "        Given an entry of a source dataset, a target dataset is searched.\n",
    "        For each pair of entries, <source, target>, a matching score is computed.\n",
    "        1st arg: The id from the entry of the source dataset.\n",
    "        2nd arg: The street number from the entry of the source dataset.\n",
    "        3rd arg: The street name from the entry of the source dataset.\n",
    "        4th arg: The postal code and city from the entry of the source dataset.\n",
    "        Returns: A dictionary containing the matches.\n",
    "        Dictionary key: Id from source dataset.\n",
    "        Dictionary value: A list where the 1st element is a matching score (1:absolute, <1: partial)\n",
    "        and the 2nd element is the matched entry (dataframe) of target dataset.\n",
    "        \"\"\"\n",
    "        source_id_code = id_code\n",
    "        source_str_num = snum\n",
    "        source_str_name = sname\n",
    "        source_pc_city = postal_code_city.replace('-', ' ')\n",
    "\n",
    "        if source_id_code in self.dataset2.id.values:\n",
    "            hit_id = self.dataset2.loc[self.dataset2['id'] == source_id_code]\n",
    "            match_score = 1.0\n",
    "            self.matches[source_id_code] = [match_score, hit_id]\n",
    "            self.full_matches += 1\n",
    "            print(\"    ID in source entry found in target dataset:\", source_id_code)\n",
    "            print(\"      [Match score: \", match_score, \"]\")\n",
    "        elif self.dataset2['address'].str.contains(source_pc_city, regex=False).any():\n",
    "            hit_id = self.dataset2[self.dataset2['address'].str.contains(source_pc_city, regex=False)]\n",
    "            street_num_name_re = re.escape(source_str_num) + \" \" + r\"[a-zA-Z]+\" +\\\n",
    "                \" \" + re.escape(source_str_name) + \",\"\n",
    "            if hit_id['address'].str.contains(street_num_name_re, regex=True).any():\n",
    "                hit_addr = hit_id[hit_id['address'].str.contains(street_num_name_re, regex=True)]\n",
    "                if self.has_one_entry(hit_addr):\n",
    "                    source_address = source_str_num + \" \" + source_str_name + \" \" + source_pc_city\n",
    "                    target_str = hit_addr['address'].to_string()\n",
    "                    match_score = self.estimate_match_score(source_address, target_str)\n",
    "                    self.matches[source_id_code] = [match_score, hit_addr]\n",
    "                    self.partial_matches = 1\n",
    "                    print(\"    Address in source entry found in target dataset:\", source_address)\n",
    "                    print(\"      [Match score: \", match_score, \"]\")\n",
    "        return self.matches\n",
    "\n",
    "    def show_matches(self) -> None:\n",
    "        \"\"\"\n",
    "        Reports (prints) the matched entries.\n",
    "        Returns: None.\n",
    "        \"\"\"\n",
    "        for mathched_id, matched_entry in self.matches.items():\n",
    "            print(\"Matched id: \", mathched_id, \" Matched entry: \")\n",
    "            print(matched_entry)\n",
    "\n",
    "    def report_match_stats(self) -> None:\n",
    "        \"\"\"\n",
    "        Reports (prints) basic statistics of matches.\n",
    "        Returns: None.\n",
    "        \"\"\"\n",
    "        print(\"Num. of absolute matches (based on ID): \", self.full_matches)\n",
    "        print(\"Num. of partial matches (based on address): \", self.partial_matches)\n",
    "\n",
    "    def match_datasets(self) -> None:\n",
    "        \"\"\"\n",
    "        Matches the entries of the two datasets.\n",
    "        Returns: None:\n",
    "        \"\"\"\n",
    "        for entry_counter, cur_entry in self.dataset1.iterrows():\n",
    "            print(\"  Index of source entry under processing: \", entry_counter)\n",
    "            postal_code_city = str(cur_entry.postal_code) + \" \" + cur_entry.city\n",
    "            self.search_target_dataset(cur_entry.id, str(cur_entry.street_number), \\\n",
    "                cur_entry.street_name, postal_code_city)\n",
    "\n",
    "    def evaluate_testonly(self) -> float:\n",
    "        \"\"\"\n",
    "        Evaluates the matches.\n",
    "        * Note: Given that no groundtruth exists,\n",
    "        this is provided for testing purposes along with the evaluation metrics.\n",
    "        Returns: F1 measure.\n",
    "        \"\"\"\n",
    "        num_correct = 0\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        for _, _ in self.matches.items():\n",
    "            if random.choice([True, False]):\n",
    "                num_correct += 1\n",
    "        precision = num_correct / len(self.matches)\n",
    "        recall = num_correct /  len(self.dataset1.index)\n",
    "        self.evaluation_f1 = (2 * precision * recall) / (precision + recall)\n",
    "        self.evaluation_f1 = (\"%.2f\" % self.evaluation_f1)\n",
    "        print(\"Precision: \", precision)\n",
    "        print(\"Recall: \", recall)\n",
    "        print(\"F1 measure: \", self.evaluation_f1)\n",
    "        return self.evaluation_f1\n",
    "\n",
    "    def run(self) -> None:\n",
    "        \"\"\"\n",
    "        The main method: iterate over first dataset (aka source)\n",
    "        searching forcmatches in the second dataset (aka target).\n",
    "        Returns: None.\n",
    "        \"\"\"\n",
    "        self.dataset1 = self.load_dataset(self.dataset1_filename)\n",
    "        self.dataset2 = self.load_dataset(self.dataset2_filename)\n",
    "        print(\"  Size of first dataset: \", self.dataset1.shape)\n",
    "        print(\"  Size of second dataset: \", self.dataset2.shape)\n",
    "        self.preprocess_datasets()\n",
    "        print(\"  Size of first dataset after pre-processing: \", self.dataset1.shape)\n",
    "        print(\"  Size of second dataset after pre-processing: \", self.dataset2.shape)\n",
    "        self.match_datasets()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
